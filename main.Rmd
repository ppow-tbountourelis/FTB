---
title: "First Time Buyers"
output: 
  html_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Scope

This document describes the scoring and subsequent segmentation of First Time Buyers (FTB). The scoring is a "forward-looking" measure, i.e., it's the propensity that the Buyer will convert in the next 90-days. The scoring is performend using a "Cox Proporsional Hazards" model. The resulting segmentation can be used for customized marketing inititiatives.


## Libraries

```{r libraries, results='hide', warning=FALSE, message=FALSE}
library(knitr)
library(survival)
library(rpart)
library(dplyr)
source("db.R")
```

## Data

The table mart_ftb is a data mart designed for this project. It contains all first time orders since 01/01/2018.

```{r readdata}
# Read Data
query <- 'select * from mart_ftb'
data <- exeQueryString (query, stringsAsFactors = TRUE)
```

We define the data used for training our model and the data corresponding to first time customers that have not placed a second order yet.

```{r datadef}
data.train <- data
data.score <- data[!is.na(data$IS_SCORE),]
data.nm    <- data[!is.na(data$NET_MARGIN_FWD_90D),]
```

## Time-to-Event Modeling

### Survival Object

We fit the survival model. The column OUTCOME_TIME is the time between the first purchase and the subsequent event. The OUTCOME_CENSORED is a binary column indicating whether the second purchase has happened.

```{r timetoevent1}
surv <- Surv(time = data.train$OUTCOME_TIME, 
            event = data.train$OUTCOME_CENSORED)
```

### Cox Proportional Hazards model

Fit the Cox proportional hazards model. We use a limited set of attributes to enhance the predictive power of the model.

```{r timetoevent2}
cox <- coxph(surv ~ QUARTER + 
                    NEW_CHANNEL +
                    USERGROUP_V2 +
                    TIME_BETWEEN_REG_FIRST, data = data.train)
cox
```

Using the fitted model we can predict the propensity of the Buyer converting in a given time period. First we calculate the cumulative baseline hazard function.

```{r timetoevent3}
curve <- basehaz(cox, centered=TRUE); names(curve) <- c("chaz", "time")
head(curve)
```

Then we calculate the baseline 30, 60 and 90 day conversion propensity.

```{r timetoevent4}
curve            <- basehaz(cox, centered=TRUE); names(curve) <- c("chaz", "time")
curve$haz        <- c(curve$chaz[1], diff(curve$chaz))
curve$conv_30    <- 1 - exp(-lead(curve$chaz, 30)) - (1 - exp(-lead(curve$chaz, 0)))
curve$conv_60    <- 1 - exp(-lead(curve$chaz, 60)) - (1 - exp(-lead(curve$chaz, 0)))
curve$conv_90    <- 1 - exp(-lead(curve$chaz, 90)) - (1 - exp(-lead(curve$chaz, 0)))
conversion <- function(time)
{
   if (time > 1000) (return (0.007422))
   i = which(curve$time == time)
   return(curve[i,]$conv_90)
}
head(curve)
```

Now we can use the baseline conversion to calculate the actual 90-day conversion propensity for each Buyer.

```{r timetoevent5}
data.score$baseline_hazard <- sapply(data.score$OUTCOME_TIME, conversion)
data.score$risk            <- predict(cox, newdata = data.score, type = 'risk')
data.score$score           <- data.score$baseline_hazard * data.score$risk
```


### Predicting the Net Margin

Predict the expected Net Margin per Buyer. We fit a linear regression model on the Net Margin of FTBs.

```{r timetoevent6}
linearMod <- lm(NET_MARGIN_FWD_90D ~ QUARTER + 
                                     NEW_CHANNEL +
                                     USERGROUP_V2 +
                                     TIME_BETWEEN_REG_FIRST, data = data.nm)
data.score$nm <- predict(linearMod, newdata = data.score)
data.score$nm_pred <- data.score$score * data.score$nm
```

In this section we calculated two Buyer scores: (i) A propensity score for converting in the next 90-days, and (ii) the expected Net Margin. We will use the latter to propose a Buyer and a Client segmentation.



## Segments 

### Buyers
We segment the Buyers using a segmentation technique based on recursive partitioning. The technique suggests three segments.

```{r timetoevent7}
data.score     <- data.score[order(data.score$nm_pred),]
data.score$rnk <- 1:nrow(data.score)
fit <- rpart(formula = score ~ rnk, 
                data = data.score, 
             control = rpart.control(cp = 0.075), 
              method = "anova"
)
fit

i <- 63189; data.score[i,]$nm_pred;
i <- 98325; data.score[i,]$nm_pred;

tags <- c("1","2", "3")
breaks <- c(0, 20.79, 57.44, 10000)
data.score$bins <- cut(data.score$nm_pred, 
                       breaks=breaks, 
                       include.lowest=TRUE, 
                       right=FALSE, 
                       labels=tags
)
data.score <- data.score[!is.na(data.score$nm_pred),]
aggregate(data.score$nm_pred, by=list(data.score$bins), FUN=mean)
aggregate(data.score$nm_pred, by=list(data.score$bins), FUN=sum)

t <- aggregate(data.score$nm_pred, by=list(data.score$bins), FUN=length)
t
t$x/sum(t$x)

a <- sort(data.score$nm_pred); plot(a)
abline(h = 20.79, col="blue")
abline(h = 57.44, col="blue")
```


### Clients

We can use the Buyer score to create Client segments. In a similar fashion as before, we use a recursive partitioning technique to create a set of client segments

```{r clientgrouping}
library(rpart)
data.score$CLIENT_ID <- as.factor(data.score$CLIENT_ID)

fit <- rpart(score ~ CLIENT_ID, 
             data   = data.score, 
             method = "anova"
)

data.score$client_group <- predict(fit, data.score)
```

## Export to the Database

The scores are attached to the data.score frame. The frame is written back to RPTMT.

```{r export}
exp <- data.score[, c("CUSTOMER_ID", "CLIENT_ID", "score", "nm_pred", "bins", "client_group")]

# Output data
#con <- openCon()
#sqlDrop(channel = con, 
#        sqtable = "mart_ftb_score", 
#        errors = FALSE)
#sqlSave(channel = con, 
#        dat =exp, 
#        tablename = "mart_ftb_score",
#        append   = FALSE,
#        rownames = FALSE, 
#        colnames = FALSE, 
#        verbose  = FALSE,
#        safer    = TRUE, 
#        addPK    = FALSE, 
#        fast     = TRUE, 
#        test     = FALSE, 
#        nastring = NULL
#        )
#close(con)
```


## Model Validation

```{r validation}
query <- 'select * from mart_ftb_test'
data.test <- exeQueryString (query, stringsAsFactors = TRUE)

data.test$baseline_hazard <- sapply(data.test$TEST_TIME, conversion)
data.test$risk            <- predict(cox, newdata = data.test, type = 'risk')
data.test$score           <- data.test$baseline_hazard * data.test$risk

data.test$nm <- predict(linearMod, newdata = data.test)
data.test$nm_pred <- data.test$score * data.test$nm

data.test$validation <- ifelse(!is.na(data.test$TEST_DAYS_TO_EVENT) & data.test$TEST_DAYS_TO_EVENT <= 90, 1, 0)

breaks <- seq(0, 1, 0.05)
data.test$score_bins <- cut(data.test$score, 
                            breaks=breaks, 
                            include.lowest=TRUE, 
                            right=FALSE 
                            #labels=tags
                            )

aggregate(data.test$validation, by=list(data.test$score_bins), FUN=mean)
```